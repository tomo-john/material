# ニュートラルネットワーク

人間の脳の神経回路(ニューロン)をまねた数理モデルで、データからパターンを学習して分類・予測などを行う仕組み。

- Neuron(ニューロン): 脳の神経細胞
- Network(ネットワーク): それらがつながったネットワーク

=> ニュートラルネットワーク = 人工の神経回路

## ニューロン(パーセプトロン)の基本構造

入力(x1～x3) => 重み(w1～w3) => バイアス(b) => 活性化関数(f) => 出力(y)

- x1, x2, x3: 入力データ(特微量)
- w1, w2, w3: 重み(どの入力をどれだけ重要視するか)
- b: バイアス(出力の調整役)
- Σ: 入力x重みを合計(線形結合)
- 活性化関数: 出力を`on/off`や`0/1`に変換
- y: 出力結果(分類や予測の値)

## 多層パーセプトロン(ニュートラルネットワーク本体)

1つのニューロンでは単純な判定しかできないので、たくさん組み合わせて層構造にしたのが本当のニュートラルネットワーク。

入力層 => 中間層(隠れ層) => 出力層

- 入力層: 特微量(画像のピクセル値、数値データなど)を受け取る
- 中間層(隠れ層): 入力から特徴を抽出する(モデルの頭脳部分)
- 出力層: 分類結果や予測値を出力する

## 学習の仕組み: 誤差逆伝播法

ニュートラルネットワークの学習は出力の誤差を逆向きに伝えて、重みを少しずつ修正することで行われる。

- 1. 順伝播(forward)

  入力 => 出力を計算

- 2. 誤差の計算

  出力と正解ラベルとの差(誤差)を計算

- 3. 逆伝播(backpropagation)

  誤差が小さくなるように重み・バイアスを調整する(勾配降下法)

- 4. 繰り返し

  これを何度も繰り返して最適な重みを見つけていく

## 活性化関数

活性化関数は出力をどう変換するかを決める関数で、ネットワークが非線形な表現を学ぶために欠かせない。

| 関数           | 形     | 特徴                                   |
|----------------|--------|----------------------------------------|
| ステップ関数   | 0 or 1 | 古典的。単純な判定に使う               |
| シグモイド関数 | 0 ～ 1 | 確率っぽい出力。初期のNNでよく使われた |
| ReLU関数       | 0 or x | 最近の主流。勾配消失が起きにくい       |

